
> library(keras)

> library(tfruns)

> library(dplyr)

> # Define hyperparameters as flags
> FLAGS <- flags(
+   flag_integer("units1", 64),
+   flag_integer("units2", 32),
+   flag_numeric("dropout1", 0.3 .... [TRUNCATED] 

> # ---------------------------
> library(insuranceData)

> data(dataOhlsson)

> data = dataOhlsson

> str(data)
'data.frame':	64548 obs. of  9 variables:
 $ agarald : int  0 4 5 5 6 9 9 9 10 10 ...
 $ kon     : Factor w/ 2 levels "K","M": 2 2 1 1 1 1 1 2 2 2 ...
 $ zon     : int  1 3 3 4 2 3 4 4 2 4 ...
 $ mcklass : int  4 6 3 1 1 3 3 4 3 2 ...
 $ fordald : int  12 9 18 25 26 8 6 20 16 17 ...
 $ bonuskl : int  1 1 1 1 1 1 1 1 1 1 ...
 $ duration: num  0.175 0 0.455 0.173 0.181 ...
 $ antskad : int  0 0 0 0 0 0 0 0 0 0 ...
 $ skadkost: int  0 0 0 0 0 0 0 0 0 0 ...

> summary(data)
    agarald      kon            zon           mcklass       fordald         bonuskl     
 Min.   : 0.00   K: 9853   Min.   :1.000   Min.   :1.0   Min.   : 0.00   Min.   :1.000  
 1st Qu.:31.00   M:54695   1st Qu.:2.000   1st Qu.:3.0   1st Qu.: 5.00   1st Qu.:2.000  
 Median :44.00             Median :3.000   Median :4.0   Median :12.00   Median :4.000  
 Mean   :42.42             Mean   :3.213   Mean   :3.7   Mean   :12.54   Mean   :4.025  
 3rd Qu.:52.00             3rd Qu.:4.000   3rd Qu.:5.0   3rd Qu.:16.00   3rd Qu.:7.000  
 Max.   :92.00             Max.   :7.000   Max.   :7.0   Max.   :99.00   Max.   :7.000  
    duration          antskad          skadkost     
 Min.   : 0.0000   Min.   :0.0000   Min.   :     0  
 1st Qu.: 0.4630   1st Qu.:0.0000   1st Qu.:     0  
 Median : 0.8274   Median :0.0000   Median :     0  
 Mean   : 1.0107   Mean   :0.0108   Mean   :   264  
 3rd Qu.: 1.0000   3rd Qu.:0.0000   3rd Qu.:     0  
 Max.   :31.3397   Max.   :2.0000   Max.   :365347  

> head(data)
  agarald kon zon mcklass fordald bonuskl duration antskad skadkost
1       0   M   1       4      12       1 0.175342       0        0
2       4   M   3       6       9       1 0.000000       0        0
3       5   K   3       3      18       1 0.454795       0        0
4       5   K   4       1      25       1 0.172603       0        0
5       6   K   2       1      26       1 0.180822       0        0
6       9   K   3       3       8       1 0.542466       0        0

> data <- data[data$skadkost > 0 & data$duration > 0, ]

> # Data split
> set.seed(123) # for reproducibility

> train_index <- createDataPartition(data$skadkost, p = 0.8, list = FALSE) # 80:20 split

> train_data <- data[train_index, ]

> test_data <- data[-train_index, ]

> # ---------------------------
> 
> # Select same variables
> vars <- c("skadkost", "agarald", "kon", "zon", "mcklass", "fordald", "bonuskl")

> train_model <- train_data %>% select(all_of(vars))

> test_model <- test_data %>% select(all_of(vars))

> # Create design matrices
> train_model <- model.matrix(skadkost ~ . -1, train_model) %>% as.data.frame()

> train_model$skadkost <- train_data$skadkost

> test_model <- model.matrix(skadkost ~ . -1, test_model) %>% as.data.frame()

> test_model$skadkost <- test_data$skadkost

> # Add log(duration)
> train_model$log_duration <- log(train_data$duration)

> test_model$log_duration <- log(test_data$duration)

> normalise_train <- function(x) (x - min(x)) / (max(x) - min(x))

> # Normalise training data
> train_features <- train_model %>% select(-skadkost) %>% mutate_all(normalise_train)

> # Capture min/max for each variable
> mins <- sapply(train_model %>% select(-skadkost), min)

> maxs <- sapply(train_model %>% select(-skadkost), max)

> # Apply same scaling to test set
> normalise_test <- function(x, varname) (x - mins[varname]) / (maxs[varname] - mins[varname])

> test_features <- test_model %>%
+   select(-skadkost) %>%
+   mutate(across(everything(), ~ normalise_test(.x, cur_column())))

> x_train <- as.matrix(train_features)

> y_train <- log(train_model$skadkost)

> x_test <- as.matrix(test_features)

> y_test <- log(test_model$skadkost)

> print(dim(x_train))
[1] 534   8

> print(dim(y_train))
NULL

> print(anyNA(x_train))
[1] FALSE

> print(anyNA(y_train))
[1] FALSE

> print(dim(x_test))
[1] 132   8

> print(dim(y_test))
NULL

> print(anyNA(x_test))
[1] FALSE

> print(anyNA(y_test))
[1] FALSE

> ncol(x_train)
[1] 8

> x_train <- np_array(x_train)

> y_train <- np_array(y_train)

> x_test <- np_array(x_test)

> y_test <- np_array(y_test)

> # ---------------------------
> # MODEL BUILDING
> # ---------------------------
> model <- keras_model_sequential()

> model$add(layer_dense(units = FLAGS$units1, activation = "relu", input_shape = c(ncol(x_train))))

> model$add(layer_dropout(rate = FLAGS$dropout1))

> model$add(layer_dense(units = FLAGS$units2, activation = "relu"))

> model$add(layer_dense(units = 1))

> model$compile(
+   loss = "mse",
+   optimizer = optimizer_adam(learning_rate = FLAGS$lr),
+   metrics = list("mean_squared_error", "mean_absolute_e ..." ... [TRUNCATED] 

> # ---------------------------
> # MODEL TRAINING
> # ---------------------------
> history <- model$fit(
+   x = x_train,
+   y = y_train,
+   epoch .... [TRUNCATED] 

> # ---------------------------
> # EVALUATE ON TRAINING SET (or on validation/test if you want)
> # ---------------------------
> scores <- model$eva .... [TRUNCATED] 

> # Predict on training data to get MAE, RMSE explicitly
> preds <- model$predict(x_train)

> preds <- as.numeric(preds)

> y_train <- as.numeric(y_train)

> str(y_train)
 num [1:534] 8.83 9.46 8.52 8.99 9.62 ...

> str(preds)
 num [1:534] 5.08 4.93 5.35 5.53 4.97 ...

> mae <- mean(abs(preds - y_train))

> rmse <- sqrt(mean((preds - y_train)^2))

> cat("MAE:", mae, "\n")
MAE: 2.321301 

> cat("RMSE:", rmse, "\n")
RMSE: 2.770333 

> # ---------------------------
> # SAVE METRICS FOR TFRUNS
> # ---------------------------
> write.csv(data.frame(mae = mae, rmse = rmse), file = "me ..." ... [TRUNCATED] 
