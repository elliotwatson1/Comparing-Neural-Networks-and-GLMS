
> library(keras)

> library(tfruns)

> library(dplyr)

> # Define custom Gamma deviance loss
> gamma_loss <- function(y_true, y_pred){
+   K <- backend()
+   y_true <- K$clip(y_true, K$epsilon(), K$cast_to .... [TRUNCATED] 

> FLAGS <- flags(
+   flag_integer("units1", 64),
+   flag_integer("units2", 32),
+   flag_integer("units3", 16), # optional third layer
+   flag_nume .... [TRUNCATED] 

> # ---------------------------
> # Build model dynamically
> # ---------------------------
> model <- keras_model_sequential()

> model$add(layer_dense(units = FLAGS$units1, activation = FLAGS$activation, input_shape = c(ncol(x_train))))

> model$add(layer_dropout(rate = FLAGS$dropout1))

> if (FLAGS$num_layers >= 2) {
+   model$add(layer_dense(units = FLAGS$units2, activation = FLAGS$activation))
+ }

> if (FLAGS$num_layers == 3) {
+   model$add(layer_dense(units = FLAGS$units3, activation = FLAGS$activation))
+ }

> # Final output layer
> model$add(layer_dense(units = 1, activation = FLAGS$final_activation))

> # Select optimizer
> opt <- switch(FLAGS$optimizer,
+               "adam" = optimizer_adam(learning_rate = FLAGS$lr),
+               "rmsprop" = o .... [TRUNCATED] 

> model$compile(
+   loss = gamma_loss,
+   optimizer = opt,
+   metrics = list("mean_squared_error", "mean_absolute_error")
+ )

> # Fix: Convert inputs/targets to correct shape
> # Convert to float32
> x_train <- np$array(x_train, dtype = "float32")
