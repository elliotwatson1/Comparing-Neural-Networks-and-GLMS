
> library(keras)

> library(tfruns)

> library(dplyr)

> # Define custom Gamma deviance loss
> gamma_loss <- function(y_true, y_pred){
+   K <- backend()
+   y_true <- K$clip(y_true, K$epsilon(), K$cast_to .... [TRUNCATED] 

> FLAGS <- flags(
+   flag_integer("units1", 64),
+   flag_integer("units2", 32),
+   flag_integer("units3", 16), # optional third layer
+   flag_nume .... [TRUNCATED] 

> # ---------------------------
> # Build model dynamically
> # ---------------------------
> model <- keras_model_sequential()

> model$add(layer_dense(units = FLAGS$units1, activation = FLAGS$activation, input_shape = c(ncol(x_train))))

> model$add(layer_dropout(rate = FLAGS$dropout1))

> if (FLAGS$num_layers >= 2) {
+   model$add(layer_dense(units = FLAGS$units2, activation = FLAGS$activation))
+ }

> if (FLAGS$num_layers == 3) {
+   model$add(layer_dense(units = FLAGS$units3, activation = FLAGS$activation))
+ }

> # Final output layer
> model$add(layer_dense(units = 1, activation = FLAGS$final_activation))

> # Select optimizer
> opt <- switch(FLAGS$optimizer,
+               "adam" = optimizer_adam(learning_rate = FLAGS$lr),
+               "rmsprop" = o .... [TRUNCATED] 

> # Switching loss function
> loss_fn <- switch(
+   FLAGS$loss_function,
+   "gamma" = gamma_loss,
+   "mse" = "mse",
+   stop("Unsupported loss func ..." ... [TRUNCATED] 

> model$compile(
+   loss = loss_fn,
+   optimizer = opt,
+   metrics = list("mean_squared_error", "mean_absolute_error")
+ )

> # Conditionally transform the targets
> if (FLAGS$loss_function == "mse") {
+   y_train <- log(train_model$skadkost)
+   y_test <- log(test_model$sk .... [TRUNCATED] 

> # Fix: Convert inputs/targets to correct shape
> x_train <- as.matrix(x_train)

> x_test <- as.matrix(x_test)

> y_train <- as.numeric(y_train)

> y_test <- as.numeric(y_test)

> # ---------------------------
> history <- model$fit(
+   x = x_train,
+   y = y_train,
+   epochs = as.integer(300),
+   batch_size = FLAGS$batch_s .... [TRUNCATED] 
